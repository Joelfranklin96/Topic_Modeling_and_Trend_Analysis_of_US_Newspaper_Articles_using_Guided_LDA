{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1452aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joel\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os  # For operating system dependent functionality\n",
    "import time  # For timing code execution\n",
    "import re  # For regular expressions\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import nltk  # For natural language processing tasks\n",
    "from nltk.corpus import stopwords  # For stopwords\n",
    "from nltk.stem import WordNetLemmatizer  # For word lemmatization\n",
    "\n",
    "# For Gensim models and functionalities\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import LdaModel, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import spacy  # For advanced natural language processing\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d92888d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time taken in mins is 1\n"
     ]
    }
   ],
   "source": [
    "# Define your directory and file name\n",
    "directory = \"D:/2_nlp\"\n",
    "file_name = \"documents_with_bigrams.xlsx\"\n",
    "full_path = os.path.join(directory, file_name)\n",
    "\n",
    "start_time = time.time()\n",
    "# Load the Excel file\n",
    "df_loaded = pd.read_excel(full_path)\n",
    "end_time = time.time()\n",
    "# Now, documents_with_bigrams_loaded contains your original list of lists structure\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "326dd7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topics_1 = {\n",
    "    \"Education & Learning\": [\"school\", \"student\", \"education\", \"university\", \"class\", \"teacher\", \"study\", \"college\", \"program\", \"course\"],\n",
    "    \"Urban Development & Infrastructure\": [\"city\", \"building\", \"street\", \"construction\", \"project\", \"development\", \"area\", \"infrastructure\", \"transport\", \"planning\"],\n",
    "    \"Health & Wellness\": [\"health\", \"hospital\", \"care\", \"medical\", \"life\", \"service\", \"well\", \"program\", \"condition\", \"treatment\"],\n",
    "    \"Economic & Business Trends\": [\"market\", \"business\", \"company\", \"industry\", \"economic\", \"price\", \"sale\", \"trade\", \"investment\", \"growth\"],\n",
    "    \"Environmental Awareness & Sustainability\": [\"environment\", \"water\", \"energy\", \"sustainable\", \"climate\", \"resource\", \"conservation\", \"green\", \"pollution\", \"recycling\"]\n",
    "}\n",
    "\n",
    "seed_topics_2 = {\n",
    "    \"Cultural & Social Life\": [\"community\", \"cultural\", \"society\", \"event\", \"tradition\", \"art\", \"music\", \"festival\", \"celebration\", \"history\"],\n",
    "    \"Technology & Innovation\": [\"development\", \"technology\", \"project\", \"research\", \"innovation\", \"system\", \"design\", \"engineering\", \"digital\", \"software\"],\n",
    "    \"Public Services & Administration\": [\"service\", \"public\", \"administration\", \"government\", \"office\", \"policy\", \"management\", \"official\", \"department\", \"agency\"],\n",
    "    \"Media & Communication\": [\"news\", \"media\", \"communication\", \"information\", \"broadcast\", \"press\", \"publication\", \"report\", \"journalism\", \"social_media\"],\n",
    "    \"Legal & Justice System\": [\"law\", \"legal\", \"court\", \"justice\", \"case\", \"judge\", \"policy\", \"rights\", \"attorney\", \"sentence\"]\n",
    "}\n",
    "\n",
    "seed_topics_3 = {\n",
    "    \"Health & Wellness\": [\"hospital\", \"health\", \"care\", \"medical\", \"treatment\", \"doctor\", \"nurse\", \"wellness\", \"disease\", \"therapy\"],\n",
    "    \"Education & Learning\": [\"school\", \"education\", \"university\", \"student\", \"teacher\", \"class\", \"learning\", \"course\", \"study\", \"research\"],\n",
    "    \"Environment & Nature\": [\"environment\", \"nature\", \"water\", \"plant\", \"animal\", \"conservation\", \"climate\", \"green\", \"earth\", \"sustainability\"],\n",
    "    \"Economy & Business\": [\"business\", \"economy\", \"market\", \"company\", \"industry\", \"trade\", \"finance\", \"investment\", \"growth\", \"development\"],\n",
    "    \"Arts & Entertainment\": [\"art\", \"music\", \"film\", \"book\", \"theater\", \"gallery\", \"entertainment\", \"performance\", \"artist\", \"exhibition\"]\n",
    "}\n",
    "seed_topics_4 = {\n",
    "    \"Technology & Innovation\": [\"system\", \"technology\", \"data\", \"research\", \"development\", \"computer\", \"digital\", \"software\", \"network\", \"innovation\"],\n",
    "    \"Travel & Tourism\": [\"travel\", \"tourism\", \"hotel\", \"trip\", \"visit\", \"destination\", \"journey\", \"tourist\", \"guide\", \"vacation\"],\n",
    "    \"Food & Cuisine\": [\"food\", \"restaurant\", \"meal\", \"cook\", \"chef\", \"cuisine\", \"dining\", \"taste\", \"dish\", \"eat\"],\n",
    "    \"Public Services & Infrastructure\": [\"service\", \"public\", \"community\", \"facility\", \"transport\", \"infrastructure\", \"project\", \"support\", \"management\", \"program\"],\n",
    "    \"Science & Exploration\": [\"science\", \"research\", \"study\", \"space\", \"earth\", \"exploration\", \"scientific\", \"experiment\", \"project\", \"discovery\"]\n",
    "}\n",
    "\n",
    "seed_topics_5 = {\n",
    "    \"Cultural & Social Issues\": [\"society\", \"culture\", \"community\", \"issue\", \"social\", \"public\", \"national\", \"discussion\", \"identity\", \"values\"],\n",
    "    \"Recreation & Leisure\": [\"park\", \"game\", \"sport\", \"leisure\", \"recreation\", \"play\", \"hobby\", \"outdoor\", \"activity\", \"entertainment\"],\n",
    "    \"Law & Justice\": [\"law\", \"justice\", \"legal\", \"court\", \"case\", \"judge\", \"right\", \"policy\", \"regulation\", \"authority\"],\n",
    "    \"Financial Markets\": [\"market\", \"finance\", \"economy\", \"investment\", \"stock\", \"bank\", \"trade\", \"financial\", \"currency\", \"capital\"],\n",
    "    \"Human Rights & Equality\": [\"right\", \"equality\", \"freedom\", \"social\", \"issue\", \"justice\", \"law\", \"policy\", \"human\", \"community\"]\n",
    "}\n",
    "total_seed_topics = [seed_topics_5, seed_topics_4, seed_topics_3, seed_topics_2, seed_topics_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f1864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(documents, num_topics, no_below, no_above, total_passes, random_state, low_value):\n",
    "    \"\"\"\n",
    "    Trains an LDA model using documents that have been preprocessed, including phrase detection and NER.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents: List of preprocessed documents, each represented as a list of tokens.\n",
    "    - num_topics: The desired number of topics.\n",
    "    - no_below: Filter out tokens that appear in fewer than 'no_below' documents.\n",
    "    - no_above: Filter out tokens that appear in more than 'no_above' proportion of documents.\n",
    "    - total_passes: Number of passes through the corpus during training.\n",
    "    - random_state: Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - lda_model: The trained LDA model.\n",
    "    - dictionary: Gensim dictionary created from the documents.\n",
    "    - corpus: Document-term matrix used for LDA training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary and filter extremes\n",
    "    dictionary = Dictionary(documents)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    \n",
    "    # Create the Document-Term Matrix\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    \n",
    "    # Apply TF-IDF filtering\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    tfidf_corpus = [[(id, freq) for id, freq in doc if tfidf.idfs[id] > low_value] for doc in corpus]\n",
    "    \n",
    "    # Initialize and train the LDA model\n",
    "    lda_model = LdaModel(corpus=tfidf_corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                         passes=total_passes, random_state=random_state)\n",
    "    \n",
    "    return lda_model, dictionary, tfidf_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46f76098",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.4840537814924571, Time: 25 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.060*\"game\" + 0.039*\"play\" + 0.012*\"sport\" + 0.009*\"national\" + 0.008*\"team\" + 0.007*\"two\" + 0.007*\"first\" + 0.006*\"new_york\" + 0.006*\"today\" + 0.005*\"park\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.018*\"stock\" + 0.012*\"market\" + 0.010*\"park\" + 0.009*\"room\" + 0.007*\"new\" + 0.007*\"national\" + 0.007*\"car\" + 0.006*\"trade\" + 0.006*\"lot\" + 0.006*\"sale\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.011*\"right\" + 0.011*\"one\" + 0.006*\"time\" + 0.005*\"said\" + 0.004*\"day\" + 0.004*\"man\" + 0.004*\"would\" + 0.004*\"many\" + 0.004*\"make\" + 0.004*\"two\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.009*\"home\" + 0.007*\"society\" + 0.007*\"john\" + 0.006*\"park\" + 0.005*\"church\" + 0.005*\"school\" + 0.005*\"member\" + 0.005*\"william\" + 0.005*\"street\" + 0.005*\"son\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.015*\"court\" + 0.014*\"case\" + 0.012*\"public\" + 0.010*\"law\" + 0.010*\"national\" + 0.009*\"said\" + 0.007*\"bank\" + 0.007*\"state\" + 0.006*\"issue\" + 0.006*\"judge\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.49671687206285287, Time: 25 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.013*\"hotel\" + 0.011*\"game\" + 0.007*\"team\" + 0.007*\"two\" + 0.006*\"first\" + 0.006*\"new_york\" + 0.005*\"cook\" + 0.005*\"play\" + 0.005*\"one\" + 0.005*\"three\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.011*\"room\" + 0.011*\"service\" + 0.008*\"new\" + 0.008*\"car\" + 0.007*\"lot\" + 0.007*\"500\" + 0.006*\"sale\" + 0.006*\"home\" + 0.006*\"phone\" + 0.006*\"350\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.010*\"one\" + 0.006*\"time\" + 0.005*\"said\" + 0.005*\"food\" + 0.004*\"would\" + 0.004*\"day\" + 0.004*\"man\" + 0.004*\"many\" + 0.004*\"good\" + 0.003*\"make\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.020*\"service\" + 0.012*\"program\" + 0.010*\"visit\" + 0.009*\"home\" + 0.007*\"john\" + 0.005*\"church\" + 0.005*\"trip\" + 0.005*\"school\" + 0.005*\"william\" + 0.005*\"son\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.014*\"public\" + 0.010*\"program\" + 0.009*\"said\" + 0.008*\"state\" + 0.007*\"service\" + 0.006*\"system\" + 0.005*\"project\" + 0.005*\"would\" + 0.005*\"today\" + 0.005*\"support\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.4689353675571197, Time: 27 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.018*\"book\" + 0.012*\"game\" + 0.009*\"art\" + 0.008*\"team\" + 0.008*\"two\" + 0.007*\"first\" + 0.007*\"class\" + 0.006*\"new_york\" + 0.006*\"one\" + 0.005*\"play\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.015*\"market\" + 0.011*\"water\" + 0.009*\"trade\" + 0.008*\"room\" + 0.008*\"green\" + 0.007*\"new\" + 0.006*\"car\" + 0.005*\"price\" + 0.005*\"lot\" + 0.005*\"500\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"one\" + 0.006*\"said\" + 0.006*\"water\" + 0.006*\"course\" + 0.005*\"would\" + 0.005*\"time\" + 0.004*\"day\" + 0.004*\"many\" + 0.003*\"man\" + 0.003*\"may\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.012*\"hospital\" + 0.010*\"home\" + 0.008*\"music\" + 0.008*\"john\" + 0.006*\"church\" + 0.005*\"son\" + 0.005*\"william\" + 0.005*\"miss\" + 0.005*\"street\" + 0.004*\"george\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.028*\"school\" + 0.019*\"business\" + 0.017*\"company\" + 0.009*\"state\" + 0.008*\"class\" + 0.007*\"said\" + 0.007*\"industry\" + 0.007*\"student\" + 0.007*\"study\" + 0.006*\"plant\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.497048021202055, Time: 24 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.012*\"one\" + 0.007*\"news\" + 0.006*\"time\" + 0.005*\"good\" + 0.004*\"day\" + 0.004*\"make\" + 0.004*\"man\" + 0.004*\"many\" + 0.004*\"case\" + 0.004*\"little\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.010*\"room\" + 0.009*\"service\" + 0.008*\"car\" + 0.008*\"new\" + 0.007*\"500\" + 0.006*\"lot\" + 0.006*\"350\" + 0.006*\"phone\" + 0.006*\"sale\" + 0.005*\"price\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.007*\"today\" + 0.006*\"event\" + 0.006*\"government\" + 0.006*\"two\" + 0.006*\"history\" + 0.006*\"official\" + 0.006*\"said\" + 0.005*\"press\" + 0.005*\"game\" + 0.005*\"one\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.018*\"service\" + 0.009*\"home\" + 0.007*\"society\" + 0.007*\"john\" + 0.005*\"club\" + 0.005*\"church\" + 0.005*\"son\" + 0.005*\"william\" + 0.004*\"member\" + 0.004*\"washington\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.021*\"court\" + 0.017*\"office\" + 0.016*\"public\" + 0.014*\"case\" + 0.013*\"law\" + 0.013*\"report\" + 0.013*\"government\" + 0.011*\"department\" + 0.010*\"service\" + 0.009*\"judge\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.4882863277024788, Time: 24 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.040*\"school\" + 0.017*\"class\" + 0.016*\"program\" + 0.013*\"college\" + 0.010*\"university\" + 0.009*\"student\" + 0.009*\"club\" + 0.009*\"green\" + 0.008*\"game\" + 0.006*\"teacher\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.035*\"sale\" + 0.034*\"price\" + 0.015*\"market\" + 0.008*\"room\" + 0.007*\"new\" + 0.007*\"trade\" + 0.007*\"condition\" + 0.007*\"water\" + 0.006*\"car\" + 0.005*\"lot\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.013*\"well\" + 0.011*\"life\" + 0.010*\"one\" + 0.007*\"water\" + 0.006*\"course\" + 0.005*\"time\" + 0.005*\"said\" + 0.004*\"would\" + 0.004*\"two\" + 0.004*\"day\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.045*\"street\" + 0.044*\"service\" + 0.028*\"city\" + 0.015*\"hospital\" + 0.011*\"home\" + 0.009*\"john\" + 0.007*\"church\" + 0.006*\"son\" + 0.006*\"william\" + 0.005*\"george\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.023*\"city\" + 0.017*\"business\" + 0.015*\"company\" + 0.013*\"building\" + 0.009*\"program\" + 0.008*\"state\" + 0.008*\"said\" + 0.007*\"area\" + 0.006*\"industry\" + 0.006*\"study\"\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "def emphasize_seed_words(document, seed_words, factor):\n",
    "    \"\"\"\n",
    "    Duplicate seed words in the document to make them more influential by a specified factor.\n",
    "    \"\"\"\n",
    "    emphasized_document = []\n",
    "    for word in document:\n",
    "        emphasized_document.append(word)\n",
    "        if word in seed_words:\n",
    "            emphasized_document.extend([word] * (factor - 1))  # Duplicate seed words by the factor\n",
    "    return emphasized_document\n",
    "\n",
    "# Update your document preparation to include seed word emphasis\n",
    "def prepare_documents(df, seed_topics, factor):\n",
    "    # Ensure all entries are strings and handle missing values\n",
    "    documents = df['documents'].fillna('').astype(str).str.split().tolist()\n",
    "    seed_words = [word for topic_words in seed_topics.values() for word in topic_words]\n",
    "    emphasized_documents = [emphasize_seed_words(doc, seed_words, factor) for doc in documents]\n",
    "    return emphasized_documents\n",
    "\n",
    "# Incorporate this step before dictionary and corpus creation in your LDA model building function\n",
    "for seed_topics in total_seed_topics:\n",
    "\n",
    "    factor = 5\n",
    "    seeded_documents = prepare_documents(df_loaded, seed_topics, 5)\n",
    "    # Set fixed hyperparameters\n",
    "    num_topics = 5\n",
    "    total_passes = 3\n",
    "\n",
    "    # Define the range of hyperparameters to explore\n",
    "    no_below = 15  # Example: Minimum document frequency\n",
    "    no_above = 0.2  # Example: Maximum document frequency proportion\n",
    "    low_value = 0.1  # TF-IDF low value cut-off\n",
    "\n",
    "    start_time_iter = time.time()  # Start time for this iteration\n",
    "\n",
    "    # Train the LDA model with the current set of hyperparameters\n",
    "\n",
    "    lda_model, dictionary, tfidf_corpus = train_lda_model(seeded_documents, num_topics=num_topics, no_below=no_below, no_above=no_above, total_passes=total_passes, random_state=100, low_value=low_value)\n",
    "\n",
    "    # Calculate Coherence Score using c_v measure\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=seeded_documents, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda_cv = coherence_model_lda.get_coherence()\n",
    "\n",
    "    end_time_iter = time.time()  # End time for this iteration\n",
    "    iter_duration = round((end_time_iter - start_time_iter) / 60)\n",
    "\n",
    "    print(f\"no_below: {no_below}, no_above: {no_above}, low_value: {low_value}, Coherence: {coherence_lda_cv}, Time: {iter_duration} minutes\")\n",
    "\n",
    "    # Print topics for the current model\n",
    "    print(\"Topics for the current model:\")\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f3b395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor: 5, no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.4784201278974032, Time: 26 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.048*\"state\" + 0.017*\"president\" + 0.016*\"government\" + 0.011*\"law\" + 0.009*\"house\" + 0.008*\"said\" + 0.008*\"time\" + 0.008*\"congress\" + 0.006*\"election\" + 0.006*\"would\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.055*\"room\" + 0.049*\"house\" + 0.034*\"home\" + 0.009*\"rent\" + 0.008*\"apartment\" + 0.007*\"new\" + 0.006*\"property\" + 0.006*\"building\" + 0.006*\"car\" + 0.005*\"lot\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.034*\"time\" + 0.021*\"man\" + 0.012*\"life\" + 0.011*\"world\" + 0.010*\"one\" + 0.005*\"thought\" + 0.004*\"said\" + 0.004*\"day\" + 0.004*\"play\" + 0.004*\"two\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.050*\"home\" + 0.031*\"game\" + 0.021*\"team\" + 0.014*\"play\" + 0.013*\"win\" + 0.011*\"season\" + 0.008*\"score\" + 0.007*\"player\" + 0.007*\"match\" + 0.006*\"coach\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.053*\"club\" + 0.049*\"school\" + 0.048*\"member\" + 0.039*\"meeting\" + 0.039*\"church\" + 0.023*\"president\" + 0.017*\"building\" + 0.010*\"community\" + 0.007*\"time\" + 0.006*\"state\"\n",
      "\n",
      "--------------------\n",
      "factor: 10, no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.4713707960728266, Time: 24 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.089*\"state\" + 0.049*\"president\" + 0.028*\"government\" + 0.019*\"law\" + 0.018*\"member\" + 0.014*\"congress\" + 0.011*\"election\" + 0.009*\"senate\" + 0.008*\"political\" + 0.007*\"policy\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.106*\"house\" + 0.090*\"room\" + 0.041*\"home\" + 0.019*\"property\" + 0.015*\"rent\" + 0.014*\"estate\" + 0.013*\"apartment\" + 0.006*\"new\" + 0.004*\"car\" + 0.004*\"lot\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.075*\"time\" + 0.041*\"man\" + 0.021*\"life\" + 0.019*\"world\" + 0.010*\"thought\" + 0.008*\"one\" + 0.007*\"idea\" + 0.006*\"mind\" + 0.005*\"reason\" + 0.005*\"said\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.089*\"club\" + 0.081*\"game\" + 0.055*\"team\" + 0.052*\"play\" + 0.045*\"building\" + 0.037*\"win\" + 0.035*\"season\" + 0.023*\"event\" + 0.021*\"score\" + 0.019*\"player\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.098*\"home\" + 0.055*\"school\" + 0.044*\"church\" + 0.043*\"member\" + 0.038*\"meeting\" + 0.005*\"john\" + 0.004*\"ceremony\" + 0.004*\"william\" + 0.004*\"son\" + 0.004*\"street\"\n",
      "\n",
      "--------------------\n",
      "factor: 15, no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.44863319855985606, Time: 25 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.140*\"state\" + 0.078*\"president\" + 0.044*\"government\" + 0.043*\"member\" + 0.039*\"world\" + 0.030*\"law\" + 0.021*\"congress\" + 0.017*\"election\" + 0.014*\"senate\" + 0.012*\"political\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.137*\"man\" + 0.135*\"house\" + 0.106*\"room\" + 0.029*\"home\" + 0.024*\"property\" + 0.017*\"rent\" + 0.016*\"estate\" + 0.015*\"apartment\" + 0.004*\"new\" + 0.003*\"lot\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.092*\"time\" + 0.026*\"life\" + 0.012*\"thought\" + 0.008*\"idea\" + 0.008*\"reason\" + 0.007*\"one\" + 0.007*\"mind\" + 0.005*\"said\" + 0.004*\"day\" + 0.003*\"today\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.099*\"club\" + 0.090*\"game\" + 0.070*\"meeting\" + 0.061*\"team\" + 0.057*\"play\" + 0.050*\"building\" + 0.041*\"win\" + 0.039*\"season\" + 0.026*\"event\" + 0.024*\"score\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.158*\"home\" + 0.081*\"school\" + 0.065*\"church\" + 0.047*\"member\" + 0.006*\"ceremony\" + 0.005*\"john\" + 0.005*\"celebration\" + 0.004*\"william\" + 0.003*\"son\" + 0.003*\"street\"\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "seed_topics = {\n",
    "    \"Sports\": [\"game\", \"team\", \"season\", \"play\", \"club\", \"win\", \"match\", \"score\", \"player\", \"coach\"],\n",
    "    \"Rentals & Real Estate\": [\"house\", \"home\", \"room\", \"property\", \"rent\", \"estate\", \"apartment\", \"building\", \"lease\", \"mortgage\"],\n",
    "    \"Philosophy & Thought\": [\"time\", \"life\", \"man\", \"world\", \"philosophy\", \"thought\", \"mind\", \"idea\", \"reason\", \"belief\"],\n",
    "    \"Community Gatherings/Events\": [\"church\", \"school\", \"event\", \"member\", \"community\", \"meeting\", \"ceremony\", \"celebration\", \"gathering\", \"festival\"],\n",
    "    \"Politics/Government\": [\"president\", \"state\", \"government\", \"senate\", \"congress\", \"election\", \"policy\", \"law\", \"political\", \"diplomacy\"]\n",
    "}\n",
    "factor_list = [5,10,15]\n",
    "for factor in factor_list:\n",
    "    seeded_documents = prepare_documents(df_loaded, seed_topics, factor)\n",
    "    # Set fixed hyperparameters\n",
    "    num_topics = 5\n",
    "    total_passes = 3\n",
    "\n",
    "    # Define the range of hyperparameters to explore\n",
    "    no_below = 15  # Example: Minimum document frequency\n",
    "    no_above = 0.2  # Example: Maximum document frequency proportion\n",
    "    low_value = 0.1  # TF-IDF low value cut-off\n",
    "\n",
    "    start_time_iter = time.time()  # Start time for this iteration\n",
    "\n",
    "    # Train the LDA model with the current set of hyperparameters\n",
    "\n",
    "    lda_model, dictionary, tfidf_corpus = train_lda_model(seeded_documents, num_topics=num_topics, no_below=no_below, no_above=no_above, total_passes=total_passes, random_state=100, low_value=low_value)\n",
    "\n",
    "    # Calculate Coherence Score using c_v measure\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=seeded_documents, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda_cv = coherence_model_lda.get_coherence()\n",
    "\n",
    "    end_time_iter = time.time()  # End time for this iteration\n",
    "    iter_duration = round((end_time_iter - start_time_iter) / 60)\n",
    "\n",
    "    print(f\"factor: {factor}, no_below: {no_below}, no_above: {no_above}, low_value: {low_value}, Coherence: {coherence_lda_cv}, Time: {iter_duration} minutes\")\n",
    "\n",
    "    # Print topics for the current model\n",
    "    print(\"Topics for the current model:\")\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bba120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor: 10, no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.47079236031672106, Time: 25 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.093*\"state\" + 0.048*\"president\" + 0.029*\"government\" + 0.020*\"law\" + 0.016*\"member\" + 0.014*\"congress\" + 0.011*\"election\" + 0.010*\"senate\" + 0.008*\"political\" + 0.008*\"policy\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.108*\"house\" + 0.093*\"room\" + 0.020*\"property\" + 0.015*\"rent\" + 0.014*\"estate\" + 0.013*\"apartment\" + 0.010*\"home\" + 0.006*\"new\" + 0.005*\"car\" + 0.005*\"lot\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.072*\"time\" + 0.040*\"man\" + 0.020*\"life\" + 0.019*\"world\" + 0.009*\"thought\" + 0.008*\"one\" + 0.007*\"idea\" + 0.005*\"mind\" + 0.005*\"reason\" + 0.005*\"said\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.168*\"home\" + 0.080*\"game\" + 0.055*\"team\" + 0.051*\"play\" + 0.037*\"win\" + 0.035*\"season\" + 0.029*\"club\" + 0.021*\"score\" + 0.019*\"player\" + 0.017*\"match\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.057*\"school\" + 0.048*\"member\" + 0.046*\"church\" + 0.043*\"club\" + 0.041*\"meeting\" + 0.008*\"community\" + 0.006*\"john\" + 0.004*\"president\" + 0.004*\"ceremony\" + 0.004*\"william\"\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "seed_topics = {\n",
    "    \"Sports\": [\"game\", \"team\", \"season\", \"play\", \"club\", \"win\", \"match\", \"score\", \"player\", \"coach\"],\n",
    "    \"Rentals & Real Estate\": [\"house\", \"home\", \"room\", \"property\", \"rent\", \"estate\", \"apartment\", \"lease\", \"mortgage\"],\n",
    "    \"Philosophy & Thought\": [\"time\", \"life\", \"man\", \"world\", \"philosophy\", \"thought\", \"mind\", \"idea\", \"reason\", \"belief\"],\n",
    "    \"Community Gatherings/Events\": [\"church\", \"school\", \"member\", \"community\", \"meeting\", \"ceremony\", \"celebration\", \"gathering\", \"festival\"],\n",
    "    \"Politics/Government\": [\"president\", \"state\", \"government\", \"senate\", \"congress\", \"election\", \"policy\", \"law\", \"political\", \"diplomacy\"]\n",
    "}\n",
    "\n",
    "factor = 10\n",
    "seeded_documents = prepare_documents(df_loaded, seed_topics, factor)\n",
    "# Set fixed hyperparameters\n",
    "num_topics = 5\n",
    "total_passes = 3\n",
    "\n",
    "# Define the range of hyperparameters to explore\n",
    "no_below = 15  # Example: Minimum document frequency\n",
    "no_above = 0.2  # Example: Maximum document frequency proportion\n",
    "low_value = 0.1  # TF-IDF low value cut-off\n",
    "\n",
    "start_time_iter = time.time()  # Start time for this iteration\n",
    "\n",
    "# Train the LDA model with the current set of hyperparameters\n",
    "\n",
    "lda_model, dictionary, tfidf_corpus = train_lda_model(seeded_documents, num_topics=num_topics, no_below=no_below, no_above=no_above, total_passes=total_passes, random_state=100, low_value=low_value)\n",
    "\n",
    "# Calculate Coherence Score using c_v measure\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=seeded_documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda_cv = coherence_model_lda.get_coherence()\n",
    "\n",
    "end_time_iter = time.time()  # End time for this iteration\n",
    "iter_duration = round((end_time_iter - start_time_iter) / 60)\n",
    "\n",
    "print(f\"factor: {factor}, no_below: {no_below}, no_above: {no_above}, low_value: {low_value}, Coherence: {coherence_lda_cv}, Time: {iter_duration} minutes\")\n",
    "\n",
    "# Print topics for the current model\n",
    "print(\"Topics for the current model:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n",
    "print(\"-\"*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
