{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "027e7aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # For operating system dependent functionality\n",
    "import time  # For timing code execution\n",
    "import re  # For regular expressions\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import nltk  # For natural language processing tasks\n",
    "from nltk.corpus import stopwords  # For stopwords\n",
    "from nltk.stem import WordNetLemmatizer  # For word lemmatization\n",
    "\n",
    "# For Gensim models and functionalities\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import LdaModel, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import spacy  # For advanced natural language processing\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94663bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"D:/2_nlp\"\n",
    "# Define the file names\n",
    "prepared_file_name_250_MB = \"Prepared_last_25_years_250_MB_dataset.xlsx\"\n",
    "# Construct the full file paths\n",
    "prepared_file_path_250_MB = os.path.join(directory, prepared_file_name_250_MB)\n",
    "\n",
    "# Read the .xlsx files into DataFrames\n",
    "prepared_last_25_years_df = pd.read_excel(prepared_file_path_250_MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfbba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time taken in mins is 0\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Ensure all entries in 'prepared_text' are treated as strings\n",
    "documents = prepared_last_25_years_df['prepared_text'].astype(str).apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Continue with filtering out words with less than 3 characters\n",
    "documents = [[word for word in doc if len(word) > 2] for doc in documents]\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94eb8f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time taken in mins is 4\n"
     ]
    }
   ],
   "source": [
    "def apply_bigram_phrase_detection(documents, min_count=2, threshold=5):\n",
    "    \"\"\"\n",
    "    Applies bigram phrase detection to a list of tokenized documents to identify\n",
    "    and mark multi-word expressions as single tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents: A list of documents, where each document is a list of tokens (words).\n",
    "    - min_count: The minimum count of phrase occurrences in the corpus to be considered\n",
    "                 for phrase detection. Phrases appearing less frequently than this\n",
    "                 threshold will be ignored. (Default: 2)\n",
    "    - threshold: The scoring threshold for forming phrases. Higher values mean that\n",
    "                 phrases need to be more frequent and specific to be formed. Lower\n",
    "                 values allow more phrases to be detected but may include less meaningful ones. (Default: 5)\n",
    "    \n",
    "    Returns:\n",
    "    - documents_with_bigrams: A new list of documents where detected bigrams are\n",
    "                              represented as single tokens joined by underscores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train the bigram Phrases model based on the input documents\n",
    "    bigram_phrases = Phrases(documents, min_count=min_count, threshold=threshold)\n",
    "    \n",
    "    # Convert the Phrases model to a more efficient Phraser for transforming documents\n",
    "    bigram = Phraser(bigram_phrases)\n",
    "    \n",
    "    # Apply the bigram Phraser to each document to form bigrams\n",
    "    documents_with_bigrams = [bigram[doc] for doc in documents]\n",
    "    \n",
    "    return documents_with_bigrams\n",
    "\n",
    "# Assuming 'documents' is a list of tokenized documents\n",
    "start_time = time.time()\n",
    "documents_with_bigrams = apply_bigram_phrase_detection(documents, min_count=2, threshold=10)\n",
    "end_time = time.time()\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a6fff",
   "metadata": {},
   "source": [
    "## Named Entity Recognition Emphasizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf021a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time taken in mins is 212\n"
     ]
    }
   ],
   "source": [
    "# Load SpaCy's NLP model for Named Entity Recognition (NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def emphasize_ner(document, nlp_model):\n",
    "    \"\"\"\n",
    "    Processes a document to identify and emphasize named entities using underscores,\n",
    "    while retaining non-entity tokens in their original form. The function creates a new\n",
    "    document representation where named entities are highlighted as single tokens,\n",
    "    facilitating their recognition in NLP tasks like topic modeling.\n",
    "    \n",
    "    Parameters:\n",
    "    - document: A list of words (tokens) constituting the original document.\n",
    "                The document should be pre-tokenized.\n",
    "    - nlp_model: A SpaCy NLP model used for Named Entity Recognition (NER).\n",
    "                 This model identifies the named entities in the document.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of tokens where named entities are emphasized by joining their constituent\n",
    "      words with underscores, and non-entity tokens are included as is.\n",
    "    \"\"\"\n",
    "    # Convert the list of tokens back into a string for NER processing with SpaCy\n",
    "    processed_text = nlp_model(\" \".join(document))\n",
    "    \n",
    "    new_doc = []  # Initialize an empty list to hold the processed tokens\n",
    "    idx = 0  # Index to keep track of our position in the processed_text\n",
    "    \n",
    "    # Iterate over the tokens in the processed text\n",
    "    while idx < len(processed_text):\n",
    "        if processed_text[idx].ent_iob != 0:  # If the token is part of an entity\n",
    "            entity = processed_text[idx].ent_type_  # Get the entity type\n",
    "            start = idx  # Mark the start of the entity\n",
    "            # Continue until we've processed all tokens belonging to this entity\n",
    "            while idx < len(processed_text) and processed_text[idx].ent_type_ == entity:\n",
    "                idx += 1\n",
    "            # Combine the tokens of the named entity with underscores\n",
    "            entity_text = '_'.join([processed_text[i].text for i in range(start, idx)])\n",
    "            new_doc.append(entity_text)  # Add the emphasized entity to new_doc\n",
    "        else:\n",
    "            # If the token is not part of an entity, add it as is\n",
    "            new_doc.append(processed_text[idx].text)\n",
    "            idx += 1  # Move to the next token\n",
    "\n",
    "    return new_doc\n",
    "\n",
    "# Apply Named Entity Recognition (NER) to emphasize entities in documents\n",
    "start_time = time.time()\n",
    "documents_with_ner = [emphasize_ner(doc, nlp) for doc in documents_with_bigrams]\n",
    "end_time = time.time()\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4475e15a",
   "metadata": {},
   "source": [
    "## Unguided LDA with NER emphasizing, Bigrams and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e117a0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.041*\"sunday\" + 0.036*\"500\" + 0.032*\"350\" + 0.026*\"evening\" + 0.026*\"friday\" + 0.017*\"thursday\" + 0.016*\"1885\" + 0.016*\"100\" + 0.014*\"1888\" + 0.012*\"1880\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.120*\"washington\" + 0.026*\"saturday\" + 0.022*\"tuesday\" + 0.022*\"year\" + 0.021*\"monday\" + 0.020*\"tomorrow\" + 0.019*\"virginia\" + 0.019*\"wednesday\" + 0.015*\"half\" + 0.015*\"week\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.091*\"one\" + 0.083*\"two\" + 0.076*\"today\" + 0.071*\"first\" + 0.043*\"three\" + 0.030*\"yesterday\" + 0.027*\"second\" + 0.024*\"four\" + 0.016*\"five\" + 0.015*\"third\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.067*\"american\" + 0.036*\"tonight\" + 0.034*\"today\" + 0.026*\"4000\" + 0.022*\"german\" + 0.020*\"one\" + 0.020*\"french\" + 0.019*\"british\" + 0.019*\"france\" + 0.018*\"london\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.032*\"june\" + 0.022*\"summer\" + 0.020*\"october\" + 0.019*\"maryland\" + 0.019*\"philadelphia\" + 0.019*\"april\" + 0.018*\"august\" + 0.018*\"september\" + 0.017*\"germany\" + 0.016*\"100000\"\n",
      "\n",
      "\n",
      "The total time taken in mins is 21\n"
     ]
    }
   ],
   "source": [
    "def train_lda_model(documents, num_topics, no_below, no_above, total_passes, random_state, low_value):\n",
    "    \"\"\"\n",
    "    Trains an LDA model using documents that have been preprocessed, including phrase detection and NER.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents: List of preprocessed documents, each represented as a list of tokens.\n",
    "    - num_topics: The desired number of topics.\n",
    "    - no_below: Filter out tokens that appear in fewer than 'no_below' documents.\n",
    "    - no_above: Filter out tokens that appear in more than 'no_above' proportion of documents.\n",
    "    - total_passes: Number of passes through the corpus during training.\n",
    "    - random_state: Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - lda_model: The trained LDA model.\n",
    "    - dictionary: Gensim dictionary created from the documents.\n",
    "    - corpus: Document-term matrix used for LDA training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary and filter extremes\n",
    "    dictionary = Dictionary(documents)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    \n",
    "    # Create the Document-Term Matrix\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    \n",
    "    # Apply TF-IDF filtering\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    tfidf_corpus = [[(id, freq) for id, freq in doc if tfidf.idfs[id] > low_value] for doc in corpus]\n",
    "    \n",
    "    # Initialize and train the LDA model\n",
    "    lda_model = LdaModel(corpus=tfidf_corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                         passes=total_passes, random_state=random_state)\n",
    "    \n",
    "    return lda_model, dictionary, tfidf_corpus\n",
    "\n",
    "start_time = time.time()\n",
    "# Train the LDA model with the preprocessed documents\n",
    "lda_model, dictionary, tfidf_corpus = train_lda_model(documents_with_ner, num_topics=5, no_below=20, no_above=0.2, total_passes=10, random_state=100, low_value=0.01)\n",
    "\n",
    "# Display the topics in the trained model\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n",
    "\n",
    "print()\n",
    "end_time = time.time()\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
