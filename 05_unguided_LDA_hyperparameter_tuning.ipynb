{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c18855d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import CoherenceModel, LdaModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef43b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"D:/2_nlp\"\n",
    "# Define the file names\n",
    "prepared_file_name_250_MB = \"Prepared_last_25_years_250_MB_dataset.xlsx\"\n",
    "# Construct the full file paths\n",
    "prepared_file_path_250_MB = os.path.join(directory, prepared_file_name_250_MB)\n",
    "\n",
    "# Read the .xlsx files into DataFrames\n",
    "prepared_last_25_years_df = pd.read_excel(prepared_file_path_250_MB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8170fe05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time taken in mins is 1\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Ensure all entries in 'prepared_text' are treated as strings\n",
    "documents = prepared_last_25_years_df['prepared_text'].astype(str).apply(lambda x: x.split()).tolist()\n",
    "\n",
    "# Continue with filtering out words with less than 3 characters\n",
    "documents = [[word for word in doc if len(word) > 2] for doc in documents]\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb45f4",
   "metadata": {},
   "source": [
    "## Unguided LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "475a220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(documents, num_topics, no_below, no_above, total_passes, random_state):\n",
    "    \"\"\"\n",
    "    Trains an LDA model using documents that have been preprocessed.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents: List of documents, where each document is a list of words.\n",
    "    - num_topics: The number of topics to discover.\n",
    "    - no_below: Minimum number of documents a word must appear in to be kept.\n",
    "    - no_above: Maximum proportion of documents a word can appear in to be kept.\n",
    "    - total_passes: The number of passes through the corpus during training.\n",
    "    - random_state: Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - lda_model: The trained LDA model.\n",
    "    - dictionary: The Gensim dictionary created from the documents.\n",
    "    - corpus: The document-term matrix generated from the documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary representation of the documents\n",
    "    dictionary = Dictionary(documents)\n",
    "    \n",
    "    # Filter out extremes to remove too rare or too common words\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    \n",
    "    # Create the Document-Term Matrix\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    \n",
    "    # Initialize the LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                         passes=total_passes, random_state=random_state, eval_every=None) \n",
    "    \n",
    "    return lda_model, dictionary, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f90f8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_above: 0.05, Coherence: 0.5438089910344249, Time: 20 mins\n",
      "\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.011*\"court\" + 0.010*\"county\" + 0.010*\"funeral\" + 0.009*\"lot\" + 0.008*\"columbia\" + 0.005*\"estate\" + 0.005*\"road\" + 0.005*\"notice\" + 0.005*\"cemetery\" + 0.005*\"late\"\n",
      "Topic: 1 \n",
      "Words: 0.005*\"guest\" + 0.004*\"oclock\" + 0.004*\"daughter\" + 0.004*\"rev\" + 0.003*\"mary\" + 0.003*\"charles\" + 0.003*\"afternoon\" + 0.003*\"friday\" + 0.003*\"james\" + 0.003*\"meet\"\n",
      "Topic: 2 \n",
      "Words: 0.006*\"cent\" + 0.004*\"government\" + 0.004*\"committee\" + 0.003*\"roosevelt\" + 0.003*\"board\" + 0.003*\"court\" + 0.003*\"bill\" + 0.003*\"tax\" + 0.003*\"business\" + 0.003*\"federal\"\n",
      "Topic: 3 \n",
      "Words: 0.008*\"bath\" + 0.008*\"car\" + 0.006*\"phone\" + 0.005*\"size\" + 0.005*\"350\" + 0.004*\"motor\" + 0.004*\"sedan\" + 0.004*\"lot\" + 0.004*\"box\" + 0.004*\"floor\"\n",
      "Topic: 4 \n",
      "Words: 0.005*\"game\" + 0.003*\"little\" + 0.003*\"team\" + 0.002*\"great\" + 0.002*\"thing\" + 0.002*\"know\" + 0.002*\"play\" + 0.002*\"much\" + 0.002*\"league\" + 0.002*\"could\"\n",
      "--------------------\n",
      "no_above: 0.06, Coherence: 0.5891890057480872, Time: 20 mins\n",
      "\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.011*\"bath\" + 0.010*\"lot\" + 0.010*\"ave\" + 0.008*\"phone\" + 0.008*\"car\" + 0.006*\"350\" + 0.006*\"sedan\" + 0.005*\"size\" + 0.005*\"motor\" + 0.005*\"18th\"\n",
      "Topic: 1 \n",
      "Words: 0.004*\"government\" + 0.004*\"committee\" + 0.003*\"roosevelt\" + 0.003*\"bill\" + 0.003*\"plan\" + 0.003*\"senator\" + 0.003*\"country\" + 0.003*\"public\" + 0.003*\"people\" + 0.002*\"labor\"\n",
      "Topic: 2 \n",
      "Words: 0.005*\"william\" + 0.005*\"evening\" + 0.005*\"guest\" + 0.004*\"mary\" + 0.004*\"oclock\" + 0.004*\"daughter\" + 0.004*\"james\" + 0.004*\"rev\" + 0.004*\"charles\" + 0.004*\"friday\"\n",
      "Topic: 3 \n",
      "Words: 0.010*\"cent\" + 0.008*\"district\" + 0.008*\"court\" + 0.007*\"county\" + 0.006*\"company\" + 0.005*\"building\" + 0.004*\"stock\" + 0.004*\"900\" + 0.004*\"price\" + 0.004*\"bank\"\n",
      "Topic: 4 \n",
      "Words: 0.005*\"game\" + 0.004*\"team\" + 0.003*\"back\" + 0.003*\"little\" + 0.003*\"like\" + 0.003*\"play\" + 0.002*\"ball\" + 0.002*\"second\" + 0.002*\"league\" + 0.002*\"season\"\n",
      "--------------------\n",
      "no_above: 0.07, Coherence: 0.647368221877526, Time: 20 mins\n",
      "\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.006*\"court\" + 0.005*\"district\" + 0.005*\"cent\" + 0.004*\"committee\" + 0.004*\"government\" + 0.004*\"county\" + 0.003*\"public\" + 0.003*\"plan\" + 0.003*\"board\" + 0.003*\"roosevelt\"\n",
      "Topic: 1 \n",
      "Words: 0.011*\"car\" + 0.008*\"sale\" + 0.007*\"motor\" + 0.006*\"price\" + 0.006*\"size\" + 0.005*\"sedan\" + 0.005*\"white\" + 0.005*\"ave\" + 0.004*\"500\" + 0.004*\"black\"\n",
      "Topic: 2 \n",
      "Words: 0.015*\"bath\" + 0.014*\"lot\" + 0.009*\"large\" + 0.008*\"water\" + 0.007*\"ave\" + 0.007*\"phone\" + 0.006*\"cup\" + 0.006*\"bed\" + 0.006*\"near\" + 0.006*\"heat\"\n",
      "Topic: 3 \n",
      "Words: 0.007*\"club\" + 0.006*\"church\" + 0.005*\"george\" + 0.005*\"william\" + 0.005*\"sunday\" + 0.004*\"evening\" + 0.004*\"guest\" + 0.004*\"oclock\" + 0.004*\"meeting\" + 0.004*\"mary\"\n",
      "Topic: 4 \n",
      "Words: 0.003*\"game\" + 0.003*\"get\" + 0.003*\"back\" + 0.003*\"way\" + 0.003*\"little\" + 0.003*\"like\" + 0.003*\"great\" + 0.002*\"world\" + 0.002*\"say\" + 0.002*\"old\"\n",
      "--------------------\n",
      "no_above: 0.08, Coherence: 0.5862974691082525, Time: 20 mins\n",
      "\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.012*\"miss\" + 0.006*\"street\" + 0.006*\"church\" + 0.006*\"club\" + 0.005*\"william\" + 0.005*\"george\" + 0.005*\"meeting\" + 0.005*\"sunday\" + 0.004*\"oclock\" + 0.004*\"guest\"\n",
      "Topic: 1 \n",
      "Words: 0.020*\"room\" + 0.010*\"lot\" + 0.009*\"bath\" + 0.009*\"car\" + 0.009*\"ave\" + 0.008*\"sale\" + 0.007*\"phone\" + 0.006*\"500\" + 0.006*\"large\" + 0.006*\"350\"\n",
      "Topic: 2 \n",
      "Words: 0.006*\"game\" + 0.004*\"team\" + 0.003*\"get\" + 0.003*\"back\" + 0.003*\"little\" + 0.003*\"play\" + 0.003*\"old\" + 0.003*\"like\" + 0.003*\"well\" + 0.003*\"way\"\n",
      "Topic: 3 \n",
      "Words: 0.006*\"war\" + 0.006*\"american\" + 0.006*\"roosevelt\" + 0.005*\"united\" + 0.005*\"president\" + 0.004*\"republican\" + 0.004*\"page\" + 0.004*\"world\" + 0.004*\"senator\" + 0.004*\"democratic\"\n",
      "Topic: 4 \n",
      "Words: 0.005*\"cent\" + 0.005*\"per\" + 0.005*\"court\" + 0.004*\"district\" + 0.003*\"public\" + 0.003*\"committee\" + 0.003*\"business\" + 0.003*\"bill\" + 0.003*\"county\" + 0.003*\"company\"\n",
      "--------------------\n",
      "no_above: 0.09, Coherence: 0.579256236781834, Time: 21 mins\n",
      "\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.005*\"president\" + 0.005*\"court\" + 0.005*\"cent\" + 0.005*\"per\" + 0.005*\"district\" + 0.004*\"government\" + 0.004*\"committee\" + 0.004*\"united\" + 0.003*\"city\" + 0.003*\"national\"\n",
      "Topic: 1 \n",
      "Words: 0.008*\"game\" + 0.006*\"team\" + 0.004*\"night\" + 0.004*\"york\" + 0.004*\"second\" + 0.004*\"league\" + 0.004*\"high\" + 0.004*\"ball\" + 0.003*\"mile\" + 0.003*\"point\"\n",
      "Topic: 2 \n",
      "Words: 0.018*\"room\" + 0.010*\"lot\" + 0.009*\"bath\" + 0.008*\"ave\" + 0.008*\"sale\" + 0.007*\"car\" + 0.006*\"phone\" + 0.006*\"500\" + 0.005*\"large\" + 0.005*\"price\"\n",
      "Topic: 3 \n",
      "Words: 0.004*\"many\" + 0.004*\"make\" + 0.004*\"get\" + 0.004*\"like\" + 0.004*\"little\" + 0.003*\"say\" + 0.003*\"well\" + 0.003*\"way\" + 0.003*\"thing\" + 0.003*\"old\"\n",
      "Topic: 4 \n",
      "Words: 0.012*\"miss\" + 0.008*\"john\" + 0.006*\"church\" + 0.006*\"club\" + 0.006*\"street\" + 0.005*\"william\" + 0.005*\"son\" + 0.005*\"held\" + 0.005*\"george\" + 0.005*\"sunday\"\n",
      "--------------------\n",
      "no_above: 0.1, Coherence: 0.5564570856769937, Time: 21 mins\n",
      "\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.004*\"many\" + 0.004*\"president\" + 0.004*\"american\" + 0.003*\"war\" + 0.003*\"people\" + 0.003*\"roosevelt\" + 0.003*\"united\" + 0.003*\"world\" + 0.003*\"make\" + 0.003*\"government\"\n",
      "Topic: 1 \n",
      "Words: 0.008*\"court\" + 0.008*\"per\" + 0.008*\"cent\" + 0.007*\"district\" + 0.005*\"county\" + 0.005*\"city\" + 0.005*\"company\" + 0.004*\"tax\" + 0.004*\"board\" + 0.004*\"sale\"\n",
      "Topic: 2 \n",
      "Words: 0.006*\"game\" + 0.004*\"team\" + 0.004*\"three\" + 0.003*\"night\" + 0.003*\"back\" + 0.003*\"get\" + 0.003*\"play\" + 0.003*\"little\" + 0.003*\"good\" + 0.003*\"second\"\n",
      "Topic: 3 \n",
      "Words: 0.021*\"room\" + 0.009*\"lot\" + 0.009*\"bath\" + 0.009*\"ave\" + 0.007*\"car\" + 0.007*\"phone\" + 0.006*\"large\" + 0.006*\"house\" + 0.006*\"good\" + 0.005*\"350\"\n",
      "Topic: 4 \n",
      "Words: 0.012*\"miss\" + 0.008*\"john\" + 0.006*\"school\" + 0.006*\"church\" + 0.006*\"club\" + 0.005*\"street\" + 0.005*\"william\" + 0.005*\"held\" + 0.005*\"son\" + 0.005*\"george\"\n",
      "--------------------\n",
      "\n",
      "Best Model's Params:  {'no_above': 0.07, 'coherence_score': 0.647368221877526, 'iteration_time_mins': 20}\n",
      "Best Coherence Score:  0.647368221877526\n",
      "Time for Best Model:  20\n"
     ]
    }
   ],
   "source": [
    "# Set fixed hyperparameters\n",
    "num_topics = 5\n",
    "total_passes = 3\n",
    "no_below = 20\n",
    "random_state = 100\n",
    "\n",
    "# Define the range of no_above to explore\n",
    "no_above_values = [0.05, 0.06, 0.07, 0.08, 0.09, 0.1]\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Loop through each value of no_above\n",
    "for no_above in no_above_values:\n",
    "    start_time_iter = time.time()  # Start time for this iteration\n",
    "    \n",
    "    # Train the LDA model for the current value of no_above\n",
    "    lda_model, dictionary, corpus = train_lda_model(documents, num_topics, no_below, no_above, total_passes, random_state)\n",
    "    \n",
    "    # Calculate Coherence Score using c_v measure\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=documents, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_lda_cv = coherence_model_lda.get_coherence()\n",
    "    \n",
    "    end_time_iter = time.time()  # End time for this iteration\n",
    "    iter_duration = round((end_time_iter - start_time_iter) / 60)  # Rounded to 0 decimal places\n",
    "    \n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'no_above': no_above,\n",
    "        'coherence_score': coherence_lda_cv,\n",
    "        'iteration_time_mins': iter_duration\n",
    "    })\n",
    "    \n",
    "    # Print the coherence score for the current model\n",
    "    print(f\"no_above: {no_above}, Coherence: {coherence_lda_cv}, Time: {iter_duration} mins\")\n",
    "    \n",
    "    # Print the topics for the current model\n",
    "    print(\"\\nTopics for the current model:\")\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print(f\"Topic: {idx} \\nWords: {topic}\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "# Find and print the best result\n",
    "best_result = max(results, key=lambda x: x['coherence_score'])\n",
    "print(\"\\nBest Model's Params: \", best_result)\n",
    "print(\"Best Coherence Score: \", best_result['coherence_score'])\n",
    "print(\"Time for Best Model: \", best_result['iteration_time_mins'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
