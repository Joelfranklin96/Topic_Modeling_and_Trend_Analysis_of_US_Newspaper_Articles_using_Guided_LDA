{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0712403f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joel\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os  # For operating system dependent functionality\n",
    "import time  # For timing code execution\n",
    "import re  # For regular expressions\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import nltk  # For natural language processing tasks\n",
    "from nltk.corpus import stopwords  # For stopwords\n",
    "from nltk.stem import WordNetLemmatizer  # For word lemmatization\n",
    "\n",
    "# For Gensim models and functionalities\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import LdaModel, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import spacy  # For advanced natural language processing\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7062ddc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time taken in mins is 1\n"
     ]
    }
   ],
   "source": [
    "# Define your directory and file name\n",
    "directory = \"D:/2_nlp\"\n",
    "file_name = \"documents_with_bigrams.xlsx\"\n",
    "full_path = os.path.join(directory, file_name)\n",
    "\n",
    "start_time = time.time()\n",
    "# Load the Excel file\n",
    "df_loaded = pd.read_excel(full_path)\n",
    "\n",
    "# Convert the 'documents' column back to a list of lists\n",
    "documents_with_bigrams = [str(doc).split(' ') for doc in df_loaded['documents']]\n",
    "end_time = time.time()\n",
    "# Now, documents_with_bigrams_loaded contains your original list of lists structure\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d972da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time taken in mins is 4\n"
     ]
    }
   ],
   "source": [
    "# Define your directory and file name\n",
    "directory = \"D:/2_nlp\"\n",
    "file_name = \"Prepared_last_25_years_250_MB_dataset.xlsx\"\n",
    "full_path = os.path.join(directory, file_name)\n",
    "start_time = time.time()\n",
    "# Load the Excel file\n",
    "entire_df_loaded = pd.read_excel(full_path)\n",
    "end_time = time.time()\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4036b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_df_loaded['date'] = pd.to_datetime(entire_df_loaded['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65b38424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>newspaper_name</th>\n",
       "      <th>edition</th>\n",
       "      <th>date</th>\n",
       "      <th>page</th>\n",
       "      <th>headline</th>\n",
       "      <th>byline</th>\n",
       "      <th>article</th>\n",
       "      <th>text_for_analysis</th>\n",
       "      <th>prepared_text</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_1960-11-10_p36_sn83045462_00280608075_196011...</td>\n",
       "      <td>Evening star.</td>\n",
       "      <td>1</td>\n",
       "      <td>1960-11-10</td>\n",
       "      <td>p36</td>\n",
       "      <td>Dark Names Lockmon ,\\njansen Giont Coaches</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SAN FRANCISCO. Nov. 10\\n(AP).-Alvin Dark made ...</td>\n",
       "      <td>Dark Names Lockmon ,\\njansen Giont Coaches SAN...</td>\n",
       "      <td>dark name lockmon jansen giont coach san franc...</td>\n",
       "      <td>1960</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2_1960-11-10_p36_sn83045462_00280608075_196011...</td>\n",
       "      <td>Evening star.</td>\n",
       "      <td>1</td>\n",
       "      <td>1960-11-10</td>\n",
       "      <td>p36</td>\n",
       "      <td>FALONEY FIGURES IN SWAP\\n\\n\\nEtcheverry Deal R...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>head last Saturday when Mon\\ntreal lost in q c...</td>\n",
       "      <td>FALONEY FIGURES IN SWAP\\n\\n\\nEtcheverry Deal R...</td>\n",
       "      <td>faloney figure swap etcheverry deal rock conod...</td>\n",
       "      <td>1960</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3_1960-11-10_p36_sn83045462_00280608075_196011...</td>\n",
       "      <td>Evening star.</td>\n",
       "      <td>1</td>\n",
       "      <td>1960-11-10</td>\n",
       "      <td>p36</td>\n",
       "      <td>Home Ice Helps\\nChicago to Gain\\nLead in Leoque</td>\n",
       "      <td>BY the Associated Press</td>\n",
       "      <td>There's just no place like\\nhome as far as the...</td>\n",
       "      <td>Home Ice Helps\\nChicago to Gain\\nLead in Leoqu...</td>\n",
       "      <td>home ice help chicago gain lead leoque there p...</td>\n",
       "      <td>1960</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4_1960-11-10_p36_sn83045462_00280608075_196011...</td>\n",
       "      <td>Evening star.</td>\n",
       "      <td>1</td>\n",
       "      <td>1960-11-10</td>\n",
       "      <td>p36</td>\n",
       "      <td>Chamberlain\\nFires In 4o\\nAgainst LA</td>\n",
       "      <td>b the Associated rfess</td>\n",
       "      <td>Wilt Chamberlain keeps on\\nscoring baskets and...</td>\n",
       "      <td>Chamberlain\\nFires In 4o\\nAgainst LA Wilt Cham...</td>\n",
       "      <td>chamberlain fire 4o la wilt chamberlain keep s...</td>\n",
       "      <td>1960</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5_1960-11-10_p36_sn83045462_00280608075_196011...</td>\n",
       "      <td>Evening star.</td>\n",
       "      <td>1</td>\n",
       "      <td>1960-11-10</td>\n",
       "      <td>p36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>raped a home run the first\\ntime he faced a ma...</td>\n",
       "      <td>raped a home run the first\\ntime he faced a m...</td>\n",
       "      <td>raped home run first time faced major league p...</td>\n",
       "      <td>1960</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          article_id newspaper_name  edition  \\\n",
       "0  1_1960-11-10_p36_sn83045462_00280608075_196011...  Evening star.        1   \n",
       "1  2_1960-11-10_p36_sn83045462_00280608075_196011...  Evening star.        1   \n",
       "2  3_1960-11-10_p36_sn83045462_00280608075_196011...  Evening star.        1   \n",
       "3  4_1960-11-10_p36_sn83045462_00280608075_196011...  Evening star.        1   \n",
       "4  5_1960-11-10_p36_sn83045462_00280608075_196011...  Evening star.        1   \n",
       "\n",
       "        date page                                           headline  \\\n",
       "0 1960-11-10  p36         Dark Names Lockmon ,\\njansen Giont Coaches   \n",
       "1 1960-11-10  p36  FALONEY FIGURES IN SWAP\\n\\n\\nEtcheverry Deal R...   \n",
       "2 1960-11-10  p36    Home Ice Helps\\nChicago to Gain\\nLead in Leoque   \n",
       "3 1960-11-10  p36               Chamberlain\\nFires In 4o\\nAgainst LA   \n",
       "4 1960-11-10  p36                                                NaN   \n",
       "\n",
       "                    byline                                            article  \\\n",
       "0                      NaN  SAN FRANCISCO. Nov. 10\\n(AP).-Alvin Dark made ...   \n",
       "1                      NaN  head last Saturday when Mon\\ntreal lost in q c...   \n",
       "2  BY the Associated Press  There's just no place like\\nhome as far as the...   \n",
       "3   b the Associated rfess  Wilt Chamberlain keeps on\\nscoring baskets and...   \n",
       "4                      NaN  raped a home run the first\\ntime he faced a ma...   \n",
       "\n",
       "                                   text_for_analysis  \\\n",
       "0  Dark Names Lockmon ,\\njansen Giont Coaches SAN...   \n",
       "1  FALONEY FIGURES IN SWAP\\n\\n\\nEtcheverry Deal R...   \n",
       "2  Home Ice Helps\\nChicago to Gain\\nLead in Leoqu...   \n",
       "3  Chamberlain\\nFires In 4o\\nAgainst LA Wilt Cham...   \n",
       "4   raped a home run the first\\ntime he faced a m...   \n",
       "\n",
       "                                       prepared_text  year  month  day  \n",
       "0  dark name lockmon jansen giont coach san franc...  1960     11   10  \n",
       "1  faloney figure swap etcheverry deal rock conod...  1960     11   10  \n",
       "2  home ice help chicago gain lead leoque there p...  1960     11   10  \n",
       "3  chamberlain fire 4o la wilt chamberlain keep s...  1960     11   10  \n",
       "4  raped home run first time faced major league p...  1960     11   10  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_df_loaded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d6ea4",
   "metadata": {},
   "source": [
    "## Optimized Unguided LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cde14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(documents, num_topics, no_below, no_above, total_passes, random_state, low_value):\n",
    "    \"\"\"\n",
    "    Trains an LDA model using documents that have been preprocessed, including phrase detection and NER.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents: List of preprocessed documents, each represented as a list of tokens.\n",
    "    - num_topics: The desired number of topics.\n",
    "    - no_below: Filter out tokens that appear in fewer than 'no_below' documents.\n",
    "    - no_above: Filter out tokens that appear in more than 'no_above' proportion of documents.\n",
    "    - total_passes: Number of passes through the corpus during training.\n",
    "    - random_state: Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - lda_model: The trained LDA model.\n",
    "    - dictionary: Gensim dictionary created from the documents.\n",
    "    - corpus: Document-term matrix used for LDA training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary and filter extremes\n",
    "    dictionary = Dictionary(documents)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    \n",
    "    # Create the Document-Term Matrix\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    \n",
    "    # Apply TF-IDF filtering\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    tfidf_corpus = [[(id, freq) for id, freq in doc if tfidf.idfs[id] > low_value] for doc in corpus]\n",
    "    \n",
    "    # Initialize and train the LDA model\n",
    "    lda_model = LdaModel(corpus=tfidf_corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                         passes=total_passes, random_state=random_state)\n",
    "    \n",
    "    return lda_model, dictionary, tfidf_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14bef8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.5292118758821858, Time: 24 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.010*\"game\" + 0.007*\"two\" + 0.007*\"one\" + 0.007*\"team\" + 0.006*\"first\" + 0.006*\"play\" + 0.005*\"new_york\" + 0.005*\"three\" + 0.004*\"season\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.011*\"room\" + 0.008*\"new\" + 0.008*\"car\" + 0.007*\"lot\" + 0.006*\"500\" + 0.006*\"home\" + 0.006*\"phone\" + 0.006*\"350\" + 0.006*\"house\" + 0.005*\"ave\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"one\" + 0.006*\"said\" + 0.005*\"would\" + 0.005*\"time\" + 0.004*\"man\" + 0.004*\"day\" + 0.004*\"many\" + 0.003*\"say\" + 0.003*\"way\" + 0.003*\"two\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"home\" + 0.007*\"john\" + 0.005*\"church\" + 0.005*\"son\" + 0.005*\"william\" + 0.005*\"school\" + 0.005*\"street\" + 0.005*\"member\" + 0.005*\"miss\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.009*\"said\" + 0.006*\"year\" + 0.005*\"new\" + 0.005*\"would\" + 0.005*\"today\" + 0.005*\"president\" + 0.004*\"district\" + 0.004*\"may\" + 0.004*\"committee\"\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# Set fixed hyperparameters\n",
    "num_topics = 5\n",
    "total_passes = 3\n",
    "random_state = 100\n",
    "# Define the range of hyperparameters to explore\n",
    "no_below = 15  # Example: Minimum document frequency\n",
    "no_above = 0.2  # Example: Maximum document frequency proportion\n",
    "low_value = 0.1  # TF-IDF low value cut-off\n",
    "\n",
    "start_time_iter = time.time()  # Start time for this iteration\n",
    "\n",
    "# Train the LDA model with the current set of hyperparameters\n",
    "\n",
    "unguided_lda_model, unguided_dictionary, unguided_tfidf_corpus = train_lda_model(documents_with_bigrams, num_topics=num_topics, no_below=no_below, no_above=no_above, total_passes=total_passes, random_state=100, low_value=low_value)\n",
    "\n",
    "# Calculate Coherence Score using c_v measure\n",
    "unguided_coherence_model_lda = CoherenceModel(model=unguided_lda_model, texts=documents_with_bigrams, dictionary=unguided_dictionary, coherence='c_v')\n",
    "coherence_lda_cv = unguided_coherence_model_lda.get_coherence()\n",
    "\n",
    "end_time_iter = time.time()  # End time for this iteration\n",
    "iter_duration = round((end_time_iter - start_time_iter) / 60)\n",
    "\n",
    "print(f\"no_below: {no_below}, no_above: {no_above}, low_value: {low_value}, Coherence: {coherence_lda_cv}, Time: {iter_duration} minutes\")\n",
    "\n",
    "# Print topics for the current model\n",
    "print(\"Topics for the current model:\")\n",
    "for idx, topic in unguided_lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7f474d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the corpus from 'prepared_text', ensuring all entries are treated as strings\n",
    "prepared_texts = entire_df_loaded['prepared_text'].astype(str).tolist()\n",
    "articles_corpus = [unguided_dictionary.doc2bow(str(doc).split()) for doc in prepared_texts]\n",
    "\n",
    "# Proceed with Steps 2 to 4 as before\n",
    "# Step 2: Get the topic distribution for each document\n",
    "topic_distributions = [unguided_lda_model.get_document_topics(bow) for bow in articles_corpus]\n",
    "\n",
    "# Step 3: Find the dominant topic for each document\n",
    "dominant_topics = [max(dist, key=lambda x: x[1])[0] for dist in topic_distributions]\n",
    "\n",
    "# Step 4: Assign the dominant topics to the DataFrame\n",
    "entire_df_loaded['dominant_topic'] = dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76cd08ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prepared_text</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dark name lockmon jansen giont coach san franc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faloney figure swap etcheverry deal rock conod...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>home ice help chicago gain lead leoque there p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chamberlain fire 4o la wilt chamberlain keep s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raped home run first time faced major league p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>gourmet salad dressing u cup finely chopped gr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2 3ounce package cream cheese teaspoon grated ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>new improved crystal clear food wrap thats eas...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>refreshing change highway departmentpar serv i...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>mobutu win nod although news congo subject cha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        prepared_text  dominant_topic\n",
       "0   dark name lockmon jansen giont coach san franc...               0\n",
       "1   faloney figure swap etcheverry deal rock conod...               0\n",
       "2   home ice help chicago gain lead leoque there p...               0\n",
       "3   chamberlain fire 4o la wilt chamberlain keep s...               0\n",
       "4   raped home run first time faced major league p...               0\n",
       "..                                                ...             ...\n",
       "95  gourmet salad dressing u cup finely chopped gr...               0\n",
       "96  2 3ounce package cream cheese teaspoon grated ...               0\n",
       "97  new improved crystal clear food wrap thats eas...               1\n",
       "98  refreshing change highway departmentpar serv i...               4\n",
       "99  mobutu win nod although news congo subject cha...               2\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_df_loaded[['prepared_text', 'dominant_topic']].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6864a5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_df_loaded.to_excel(\"unguided_lda_dominant_topic_classification.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
