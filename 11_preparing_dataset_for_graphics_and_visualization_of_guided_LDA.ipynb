{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ac7588b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joel\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os  # For operating system dependent functionality\n",
    "import time  # For timing code execution\n",
    "import re  # For regular expressions\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import nltk  # For natural language processing tasks\n",
    "from nltk.corpus import stopwords  # For stopwords\n",
    "from nltk.stem import WordNetLemmatizer  # For word lemmatization\n",
    "\n",
    "# For Gensim models and functionalities\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import LdaModel, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import spacy  # For advanced natural language processing\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08210693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time taken in mins is 1\n"
     ]
    }
   ],
   "source": [
    "# Define your directory and file name\n",
    "directory = \"D:/2_nlp\"\n",
    "file_name = \"documents_with_bigrams.xlsx\"\n",
    "full_path = os.path.join(directory, file_name)\n",
    "\n",
    "start_time = time.time()\n",
    "# Load the Excel file\n",
    "df_loaded = pd.read_excel(full_path)\n",
    "\n",
    "# Convert the 'documents' column back to a list of lists\n",
    "documents_with_bigrams = [str(doc).split(' ') for doc in df_loaded['documents']]\n",
    "end_time = time.time()\n",
    "# Now, documents_with_bigrams_loaded contains your original list of lists structure\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "034016c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time taken in mins is 4\n"
     ]
    }
   ],
   "source": [
    "# Define your directory and file name\n",
    "directory = \"D:/2_nlp\"\n",
    "file_name = \"Prepared_last_25_years_250_MB_dataset.xlsx\"\n",
    "full_path = os.path.join(directory, file_name)\n",
    "start_time = time.time()\n",
    "# Load the Excel file\n",
    "entire_df_loaded = pd.read_excel(full_path)\n",
    "end_time = time.time()\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cba95836",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_df_loaded['date'] = pd.to_datetime(entire_df_loaded['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8666d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(documents, num_topics, no_below, no_above, total_passes, random_state, low_value):\n",
    "    \"\"\"\n",
    "    Trains an LDA model using documents that have been preprocessed, including phrase detection and NER.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents: List of preprocessed documents, each represented as a list of tokens.\n",
    "    - num_topics: The desired number of topics.\n",
    "    - no_below: Filter out tokens that appear in fewer than 'no_below' documents.\n",
    "    - no_above: Filter out tokens that appear in more than 'no_above' proportion of documents.\n",
    "    - total_passes: Number of passes through the corpus during training.\n",
    "    - random_state: Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - lda_model: The trained LDA model.\n",
    "    - dictionary: Gensim dictionary created from the documents.\n",
    "    - corpus: Document-term matrix used for LDA training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary and filter extremes\n",
    "    dictionary = Dictionary(documents)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    \n",
    "    # Create the Document-Term Matrix\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    \n",
    "    # Apply TF-IDF filtering\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    tfidf_corpus = [[(id, freq) for id, freq in doc if tfidf.idfs[id] > low_value] for doc in corpus]\n",
    "    \n",
    "    # Initialize and train the LDA model\n",
    "    lda_model = LdaModel(corpus=tfidf_corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                         passes=total_passes, random_state=random_state)\n",
    "    \n",
    "    return lda_model, dictionary, tfidf_corpus\n",
    "\n",
    "def emphasize_seed_words(document, seed_words, factor):\n",
    "    \"\"\"\n",
    "    Duplicate seed words in the document to make them more influential by a specified factor.\n",
    "    \"\"\"\n",
    "    emphasized_document = []\n",
    "    for word in document:\n",
    "        emphasized_document.append(word)\n",
    "        if word in seed_words:\n",
    "            emphasized_document.extend([word] * (factor - 1))  # Duplicate seed words by the factor\n",
    "    return emphasized_document\n",
    "\n",
    "# Update your document preparation to include seed word emphasis\n",
    "def prepare_documents(df, seed_topics, factor):\n",
    "    # Ensure all entries are strings and handle missing values\n",
    "    documents = df['documents'].fillna('').astype(str).str.split().tolist()\n",
    "    seed_words = [word for topic_words in seed_topics.values() for word in topic_words]\n",
    "    emphasized_documents = [emphasize_seed_words(doc, seed_words, factor) for doc in documents]\n",
    "    return emphasized_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4006020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factor: 10, no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.4713707960728266, Time: 25 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.089*\"state\" + 0.049*\"president\" + 0.028*\"government\" + 0.019*\"law\" + 0.018*\"member\" + 0.014*\"congress\" + 0.011*\"election\" + 0.009*\"senate\" + 0.008*\"political\" + 0.007*\"policy\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.106*\"house\" + 0.090*\"room\" + 0.041*\"home\" + 0.019*\"property\" + 0.015*\"rent\" + 0.014*\"estate\" + 0.013*\"apartment\" + 0.006*\"new\" + 0.004*\"car\" + 0.004*\"lot\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.075*\"time\" + 0.041*\"man\" + 0.021*\"life\" + 0.019*\"world\" + 0.010*\"thought\" + 0.008*\"one\" + 0.007*\"idea\" + 0.006*\"mind\" + 0.005*\"reason\" + 0.005*\"said\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.089*\"club\" + 0.081*\"game\" + 0.055*\"team\" + 0.052*\"play\" + 0.045*\"building\" + 0.037*\"win\" + 0.035*\"season\" + 0.023*\"event\" + 0.021*\"score\" + 0.019*\"player\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.098*\"home\" + 0.055*\"school\" + 0.044*\"church\" + 0.043*\"member\" + 0.038*\"meeting\" + 0.005*\"john\" + 0.004*\"ceremony\" + 0.004*\"william\" + 0.004*\"son\" + 0.004*\"street\"\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "seed_topics = {\n",
    "    \"Sports\": [\"game\", \"team\", \"season\", \"play\", \"club\", \"win\", \"match\", \"score\", \"player\", \"coach\"],\n",
    "    \"Rentals & Real Estate\": [\"house\", \"home\", \"room\", \"property\", \"rent\", \"estate\", \"apartment\", \"building\", \"lease\", \"mortgage\"],\n",
    "    \"Philosophy & Thought\": [\"time\", \"life\", \"man\", \"world\", \"philosophy\", \"thought\", \"mind\", \"idea\", \"reason\", \"belief\"],\n",
    "    \"Community Gatherings/Events\": [\"church\", \"school\", \"event\", \"member\", \"community\", \"meeting\", \"ceremony\", \"celebration\", \"gathering\", \"festival\"],\n",
    "    \"Politics/Government\": [\"president\", \"state\", \"government\", \"senate\", \"congress\", \"election\", \"policy\", \"law\", \"political\", \"diplomacy\"]\n",
    "}\n",
    "factor = 10\n",
    "\n",
    "seeded_documents = prepare_documents(df_loaded, seed_topics, factor)\n",
    "# Set fixed hyperparameters\n",
    "num_topics = 5\n",
    "total_passes = 3\n",
    "\n",
    "# Define the range of hyperparameters to explore\n",
    "no_below = 15  # Example: Minimum document frequency\n",
    "no_above = 0.2  # Example: Maximum document frequency proportion\n",
    "low_value = 0.1  # TF-IDF low value cut-off\n",
    "\n",
    "start_time_iter = time.time()  # Start time for this iteration\n",
    "\n",
    "# Train the LDA model with the current set of hyperparameters\n",
    "\n",
    "lda_model, dictionary, tfidf_corpus = train_lda_model(seeded_documents, num_topics=num_topics, no_below=no_below, no_above=no_above, total_passes=total_passes, random_state=100, low_value=low_value)\n",
    "\n",
    "# Calculate Coherence Score using c_v measure\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=seeded_documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda_cv = coherence_model_lda.get_coherence()\n",
    "\n",
    "end_time_iter = time.time()  # End time for this iteration\n",
    "iter_duration = round((end_time_iter - start_time_iter) / 60)\n",
    "\n",
    "print(f\"factor: {factor}, no_below: {no_below}, no_above: {no_above}, low_value: {low_value}, Coherence: {coherence_lda_cv}, Time: {iter_duration} minutes\")\n",
    "\n",
    "# Print topics for the current model\n",
    "print(\"Topics for the current model:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n",
    "print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5405fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Prepare the corpus from 'prepared_text', ensuring all entries are treated as strings\n",
    "prepared_texts = entire_df_loaded['prepared_text'].astype(str).tolist()\n",
    "articles_corpus = [dictionary.doc2bow(str(doc).split()) for doc in prepared_texts]\n",
    "\n",
    "# Proceed with Steps 2 to 4 as before\n",
    "# Step 2: Get the topic distribution for each document\n",
    "topic_distributions = [lda_model.get_document_topics(bow) for bow in articles_corpus]\n",
    "\n",
    "# Step 3: Find the dominant topic for each document\n",
    "dominant_topics = [max(dist, key=lambda x: x[1])[0] for dist in topic_distributions]\n",
    "\n",
    "# Step 4: Assign the dominant topics to the DataFrame\n",
    "entire_df_loaded['dominant_topic'] = dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e21efda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prepared_text</th>\n",
       "      <th>dominant_topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dark name lockmon jansen giont coach san franc...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>faloney figure swap etcheverry deal rock conod...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>home ice help chicago gain lead leoque there p...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chamberlain fire 4o la wilt chamberlain keep s...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>raped home run first time faced major league p...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>gourmet salad dressing u cup finely chopped gr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2 3ounce package cream cheese teaspoon grated ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>new improved crystal clear food wrap thats eas...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>refreshing change highway departmentpar serv i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>mobutu win nod although news congo subject cha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        prepared_text  dominant_topic\n",
       "0   dark name lockmon jansen giont coach san franc...               3\n",
       "1   faloney figure swap etcheverry deal rock conod...               3\n",
       "2   home ice help chicago gain lead leoque there p...               3\n",
       "3   chamberlain fire 4o la wilt chamberlain keep s...               3\n",
       "4   raped home run first time faced major league p...               3\n",
       "..                                                ...             ...\n",
       "95  gourmet salad dressing u cup finely chopped gr...               2\n",
       "96  2 3ounce package cream cheese teaspoon grated ...               1\n",
       "97  new improved crystal clear food wrap thats eas...               2\n",
       "98  refreshing change highway departmentpar serv i...               0\n",
       "99  mobutu win nod although news congo subject cha...               0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, entire_df_loaded includes a 'dominant_topic' column with the dominant topic for each document\n",
    "entire_df_loaded[['prepared_text', 'dominant_topic']].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d92a9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_df_loaded.to_excel(\"guided_lda_dominant_topic_classification.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
