{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c94ef37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # For operating system dependent functionality\n",
    "import time  # For timing code execution\n",
    "import re  # For regular expressions\n",
    "\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import nltk  # For natural language processing tasks\n",
    "from nltk.corpus import stopwords  # For stopwords\n",
    "from nltk.stem import WordNetLemmatizer  # For word lemmatization\n",
    "\n",
    "# For Gensim models and functionalities\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import LdaModel, TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import spacy  # For advanced natural language processing\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f7ffa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total time taken in mins is 1\n"
     ]
    }
   ],
   "source": [
    "# Define your directory and file name\n",
    "directory = \"D:/2_nlp\"\n",
    "file_name = \"documents_with_bigrams.xlsx\"\n",
    "full_path = os.path.join(directory, file_name)\n",
    "\n",
    "start_time = time.time()\n",
    "# Load the Excel file\n",
    "df_loaded = pd.read_excel(full_path)\n",
    "\n",
    "# Convert the 'documents' column back to a list of lists\n",
    "documents_with_bigrams = [str(doc).split(' ') for doc in df_loaded['documents']]\n",
    "end_time = time.time()\n",
    "# Now, documents_with_bigrams_loaded contains your original list of lists structure\n",
    "print(\"The total time taken in mins is {}\".format(round((end_time - start_time) / 60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d47e808c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lda_model(documents, num_topics, no_below, no_above, total_passes, random_state, low_value):\n",
    "    \"\"\"\n",
    "    Trains an LDA model using documents that have been preprocessed, including phrase detection and NER.\n",
    "    \n",
    "    Parameters:\n",
    "    - documents: List of preprocessed documents, each represented as a list of tokens.\n",
    "    - num_topics: The desired number of topics.\n",
    "    - no_below: Filter out tokens that appear in fewer than 'no_below' documents.\n",
    "    - no_above: Filter out tokens that appear in more than 'no_above' proportion of documents.\n",
    "    - total_passes: Number of passes through the corpus during training.\n",
    "    - random_state: Seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - lda_model: The trained LDA model.\n",
    "    - dictionary: Gensim dictionary created from the documents.\n",
    "    - corpus: Document-term matrix used for LDA training.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a dictionary and filter extremes\n",
    "    dictionary = Dictionary(documents)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above)\n",
    "    \n",
    "    # Create the Document-Term Matrix\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "    \n",
    "    # Apply TF-IDF filtering\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    tfidf_corpus = [[(id, freq) for id, freq in doc if tfidf.idfs[id] > low_value] for doc in corpus]\n",
    "    \n",
    "    # Initialize and train the LDA model\n",
    "    lda_model = LdaModel(corpus=tfidf_corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                         passes=total_passes, random_state=random_state)\n",
    "    \n",
    "    return lda_model, dictionary, tfidf_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b29eedb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_below: 15, no_above: 0.1, low_value: 0.01, Coherence: 0.5756896896685613, Time: 24 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.011*\"john\" + 0.008*\"son\" + 0.008*\"william\" + 0.007*\"street\" + 0.006*\"george\" + 0.006*\"miss\" + 0.006*\"james\" + 0.006*\"mary\" + 0.005*\"daughter\" + 0.005*\"charles\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.004*\"would\" + 0.004*\"man\" + 0.004*\"good\" + 0.003*\"make\" + 0.003*\"many\" + 0.003*\"way\" + 0.003*\"little\" + 0.003*\"get\" + 0.003*\"say\" + 0.003*\"made\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"club\" + 0.008*\"game\" + 0.006*\"school\" + 0.006*\"washington\" + 0.005*\"team\" + 0.005*\"member\" + 0.005*\"meeting\" + 0.005*\"new_york\" + 0.004*\"church\" + 0.004*\"held\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"room\" + 0.007*\"car\" + 0.007*\"lot\" + 0.007*\"500\" + 0.006*\"sale\" + 0.006*\"350\" + 0.006*\"phone\" + 0.005*\"house\" + 0.005*\"ave\" + 0.005*\"price\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.006*\"would\" + 0.004*\"president\" + 0.004*\"government\" + 0.004*\"work\" + 0.004*\"city\" + 0.004*\"per_cent\" + 0.004*\"made\" + 0.003*\"united_state\" + 0.003*\"committee\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.1, low_value: 0.05, Coherence: 0.5756896896685613, Time: 24 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.011*\"john\" + 0.008*\"son\" + 0.008*\"william\" + 0.007*\"street\" + 0.006*\"george\" + 0.006*\"miss\" + 0.006*\"james\" + 0.006*\"mary\" + 0.005*\"daughter\" + 0.005*\"charles\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.004*\"would\" + 0.004*\"man\" + 0.004*\"good\" + 0.003*\"make\" + 0.003*\"many\" + 0.003*\"way\" + 0.003*\"little\" + 0.003*\"get\" + 0.003*\"say\" + 0.003*\"made\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"club\" + 0.008*\"game\" + 0.006*\"school\" + 0.006*\"washington\" + 0.005*\"team\" + 0.005*\"member\" + 0.005*\"meeting\" + 0.005*\"new_york\" + 0.004*\"church\" + 0.004*\"held\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"room\" + 0.007*\"car\" + 0.007*\"lot\" + 0.007*\"500\" + 0.006*\"sale\" + 0.006*\"350\" + 0.006*\"phone\" + 0.005*\"house\" + 0.005*\"ave\" + 0.005*\"price\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.006*\"would\" + 0.004*\"president\" + 0.004*\"government\" + 0.004*\"work\" + 0.004*\"city\" + 0.004*\"per_cent\" + 0.004*\"made\" + 0.003*\"united_state\" + 0.003*\"committee\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.1, low_value: 0.1, Coherence: 0.5756896896685613, Time: 25 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.011*\"john\" + 0.008*\"son\" + 0.008*\"william\" + 0.007*\"street\" + 0.006*\"george\" + 0.006*\"miss\" + 0.006*\"james\" + 0.006*\"mary\" + 0.005*\"daughter\" + 0.005*\"charles\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.004*\"would\" + 0.004*\"man\" + 0.004*\"good\" + 0.003*\"make\" + 0.003*\"many\" + 0.003*\"way\" + 0.003*\"little\" + 0.003*\"get\" + 0.003*\"say\" + 0.003*\"made\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"club\" + 0.008*\"game\" + 0.006*\"school\" + 0.006*\"washington\" + 0.005*\"team\" + 0.005*\"member\" + 0.005*\"meeting\" + 0.005*\"new_york\" + 0.004*\"church\" + 0.004*\"held\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"room\" + 0.007*\"car\" + 0.007*\"lot\" + 0.007*\"500\" + 0.006*\"sale\" + 0.006*\"350\" + 0.006*\"phone\" + 0.005*\"house\" + 0.005*\"ave\" + 0.005*\"price\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.006*\"would\" + 0.004*\"president\" + 0.004*\"government\" + 0.004*\"work\" + 0.004*\"city\" + 0.004*\"per_cent\" + 0.004*\"made\" + 0.003*\"united_state\" + 0.003*\"committee\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.2, low_value: 0.01, Coherence: 0.5292118758821858, Time: 24 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.010*\"game\" + 0.007*\"two\" + 0.007*\"one\" + 0.007*\"team\" + 0.006*\"first\" + 0.006*\"play\" + 0.005*\"new_york\" + 0.005*\"three\" + 0.004*\"season\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.011*\"room\" + 0.008*\"new\" + 0.008*\"car\" + 0.007*\"lot\" + 0.006*\"500\" + 0.006*\"home\" + 0.006*\"phone\" + 0.006*\"350\" + 0.006*\"house\" + 0.005*\"ave\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"one\" + 0.006*\"said\" + 0.005*\"would\" + 0.005*\"time\" + 0.004*\"man\" + 0.004*\"day\" + 0.004*\"many\" + 0.003*\"say\" + 0.003*\"way\" + 0.003*\"two\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"home\" + 0.007*\"john\" + 0.005*\"church\" + 0.005*\"son\" + 0.005*\"william\" + 0.005*\"school\" + 0.005*\"street\" + 0.005*\"member\" + 0.005*\"miss\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.009*\"said\" + 0.006*\"year\" + 0.005*\"new\" + 0.005*\"would\" + 0.005*\"today\" + 0.005*\"president\" + 0.004*\"district\" + 0.004*\"may\" + 0.004*\"committee\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.2, low_value: 0.05, Coherence: 0.5292118758821858, Time: 26 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.010*\"game\" + 0.007*\"two\" + 0.007*\"one\" + 0.007*\"team\" + 0.006*\"first\" + 0.006*\"play\" + 0.005*\"new_york\" + 0.005*\"three\" + 0.004*\"season\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.011*\"room\" + 0.008*\"new\" + 0.008*\"car\" + 0.007*\"lot\" + 0.006*\"500\" + 0.006*\"home\" + 0.006*\"phone\" + 0.006*\"350\" + 0.006*\"house\" + 0.005*\"ave\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"one\" + 0.006*\"said\" + 0.005*\"would\" + 0.005*\"time\" + 0.004*\"man\" + 0.004*\"day\" + 0.004*\"many\" + 0.003*\"say\" + 0.003*\"way\" + 0.003*\"two\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"home\" + 0.007*\"john\" + 0.005*\"church\" + 0.005*\"son\" + 0.005*\"william\" + 0.005*\"school\" + 0.005*\"street\" + 0.005*\"member\" + 0.005*\"miss\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.009*\"said\" + 0.006*\"year\" + 0.005*\"new\" + 0.005*\"would\" + 0.005*\"today\" + 0.005*\"president\" + 0.004*\"district\" + 0.004*\"may\" + 0.004*\"committee\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.2, low_value: 0.1, Coherence: 0.5292118758821858, Time: 25 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.010*\"game\" + 0.007*\"two\" + 0.007*\"one\" + 0.007*\"team\" + 0.006*\"first\" + 0.006*\"play\" + 0.005*\"new_york\" + 0.005*\"three\" + 0.004*\"season\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.011*\"room\" + 0.008*\"new\" + 0.008*\"car\" + 0.007*\"lot\" + 0.006*\"500\" + 0.006*\"home\" + 0.006*\"phone\" + 0.006*\"350\" + 0.006*\"house\" + 0.005*\"ave\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"one\" + 0.006*\"said\" + 0.005*\"would\" + 0.005*\"time\" + 0.004*\"man\" + 0.004*\"day\" + 0.004*\"many\" + 0.003*\"say\" + 0.003*\"way\" + 0.003*\"two\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"home\" + 0.007*\"john\" + 0.005*\"church\" + 0.005*\"son\" + 0.005*\"william\" + 0.005*\"school\" + 0.005*\"street\" + 0.005*\"member\" + 0.005*\"miss\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.009*\"said\" + 0.006*\"year\" + 0.005*\"new\" + 0.005*\"would\" + 0.005*\"today\" + 0.005*\"president\" + 0.004*\"district\" + 0.004*\"may\" + 0.004*\"committee\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.3, low_value: 0.01, Coherence: 0.5292118758821858, Time: 24 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.010*\"game\" + 0.007*\"two\" + 0.007*\"one\" + 0.007*\"team\" + 0.006*\"first\" + 0.006*\"play\" + 0.005*\"new_york\" + 0.005*\"three\" + 0.004*\"season\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.011*\"room\" + 0.008*\"new\" + 0.008*\"car\" + 0.007*\"lot\" + 0.006*\"500\" + 0.006*\"home\" + 0.006*\"phone\" + 0.006*\"350\" + 0.006*\"house\" + 0.005*\"ave\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"one\" + 0.006*\"said\" + 0.005*\"would\" + 0.005*\"time\" + 0.004*\"man\" + 0.004*\"day\" + 0.004*\"many\" + 0.003*\"say\" + 0.003*\"way\" + 0.003*\"two\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"home\" + 0.007*\"john\" + 0.005*\"church\" + 0.005*\"son\" + 0.005*\"william\" + 0.005*\"school\" + 0.005*\"street\" + 0.005*\"member\" + 0.005*\"miss\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.009*\"said\" + 0.006*\"year\" + 0.005*\"new\" + 0.005*\"would\" + 0.005*\"today\" + 0.005*\"president\" + 0.004*\"district\" + 0.004*\"may\" + 0.004*\"committee\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.3, low_value: 0.05, Coherence: 0.5292118758821858, Time: 25 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.010*\"game\" + 0.007*\"two\" + 0.007*\"one\" + 0.007*\"team\" + 0.006*\"first\" + 0.006*\"play\" + 0.005*\"new_york\" + 0.005*\"three\" + 0.004*\"season\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.011*\"room\" + 0.008*\"new\" + 0.008*\"car\" + 0.007*\"lot\" + 0.006*\"500\" + 0.006*\"home\" + 0.006*\"phone\" + 0.006*\"350\" + 0.006*\"house\" + 0.005*\"ave\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"one\" + 0.006*\"said\" + 0.005*\"would\" + 0.005*\"time\" + 0.004*\"man\" + 0.004*\"day\" + 0.004*\"many\" + 0.003*\"say\" + 0.003*\"way\" + 0.003*\"two\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"home\" + 0.007*\"john\" + 0.005*\"church\" + 0.005*\"son\" + 0.005*\"william\" + 0.005*\"school\" + 0.005*\"street\" + 0.005*\"member\" + 0.005*\"miss\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.009*\"said\" + 0.006*\"year\" + 0.005*\"new\" + 0.005*\"would\" + 0.005*\"today\" + 0.005*\"president\" + 0.004*\"district\" + 0.004*\"may\" + 0.004*\"committee\"\n",
      "\n",
      "--------------------\n",
      "no_below: 15, no_above: 0.3, low_value: 0.1, Coherence: 0.5292118758821858, Time: 24 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.010*\"game\" + 0.007*\"two\" + 0.007*\"one\" + 0.007*\"team\" + 0.006*\"first\" + 0.006*\"play\" + 0.005*\"new_york\" + 0.005*\"three\" + 0.004*\"season\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.011*\"room\" + 0.008*\"new\" + 0.008*\"car\" + 0.007*\"lot\" + 0.006*\"500\" + 0.006*\"home\" + 0.006*\"phone\" + 0.006*\"350\" + 0.006*\"house\" + 0.005*\"ave\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"one\" + 0.006*\"said\" + 0.005*\"would\" + 0.005*\"time\" + 0.004*\"man\" + 0.004*\"day\" + 0.004*\"many\" + 0.003*\"say\" + 0.003*\"way\" + 0.003*\"two\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"home\" + 0.007*\"john\" + 0.005*\"church\" + 0.005*\"son\" + 0.005*\"william\" + 0.005*\"school\" + 0.005*\"street\" + 0.005*\"member\" + 0.005*\"miss\" + 0.004*\"club\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.009*\"said\" + 0.006*\"year\" + 0.005*\"new\" + 0.005*\"would\" + 0.005*\"today\" + 0.005*\"president\" + 0.004*\"district\" + 0.004*\"may\" + 0.004*\"committee\"\n",
      "\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no_below: 20, no_above: 0.1, low_value: 0.01, Coherence: 0.5756896896685613, Time: 25 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.011*\"john\" + 0.008*\"son\" + 0.008*\"william\" + 0.007*\"street\" + 0.006*\"george\" + 0.006*\"miss\" + 0.006*\"james\" + 0.006*\"mary\" + 0.005*\"daughter\" + 0.005*\"charles\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.004*\"would\" + 0.004*\"man\" + 0.004*\"good\" + 0.003*\"make\" + 0.003*\"many\" + 0.003*\"way\" + 0.003*\"little\" + 0.003*\"get\" + 0.003*\"say\" + 0.003*\"made\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"club\" + 0.008*\"game\" + 0.006*\"school\" + 0.006*\"washington\" + 0.005*\"team\" + 0.005*\"member\" + 0.005*\"meeting\" + 0.005*\"new_york\" + 0.004*\"church\" + 0.004*\"held\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"room\" + 0.007*\"car\" + 0.007*\"lot\" + 0.007*\"500\" + 0.006*\"sale\" + 0.006*\"350\" + 0.006*\"phone\" + 0.005*\"house\" + 0.005*\"ave\" + 0.005*\"price\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.006*\"would\" + 0.004*\"president\" + 0.004*\"government\" + 0.004*\"work\" + 0.004*\"city\" + 0.004*\"per_cent\" + 0.004*\"made\" + 0.003*\"united_state\" + 0.003*\"committee\"\n",
      "\n",
      "--------------------\n",
      "no_below: 20, no_above: 0.1, low_value: 0.05, Coherence: 0.5756896896685613, Time: 25 minutes\n",
      "Topics for the current model:\n",
      "Topic: 0 \n",
      "Words: 0.011*\"john\" + 0.008*\"son\" + 0.008*\"william\" + 0.007*\"street\" + 0.006*\"george\" + 0.006*\"miss\" + 0.006*\"james\" + 0.006*\"mary\" + 0.005*\"daughter\" + 0.005*\"charles\"\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.004*\"would\" + 0.004*\"man\" + 0.004*\"good\" + 0.003*\"make\" + 0.003*\"many\" + 0.003*\"way\" + 0.003*\"little\" + 0.003*\"get\" + 0.003*\"say\" + 0.003*\"made\"\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.009*\"club\" + 0.008*\"game\" + 0.006*\"school\" + 0.006*\"washington\" + 0.005*\"team\" + 0.005*\"member\" + 0.005*\"meeting\" + 0.005*\"new_york\" + 0.004*\"church\" + 0.004*\"held\"\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.010*\"room\" + 0.007*\"car\" + 0.007*\"lot\" + 0.007*\"500\" + 0.006*\"sale\" + 0.006*\"350\" + 0.006*\"phone\" + 0.005*\"house\" + 0.005*\"ave\" + 0.005*\"price\"\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.009*\"state\" + 0.006*\"would\" + 0.004*\"president\" + 0.004*\"government\" + 0.004*\"work\" + 0.004*\"city\" + 0.004*\"per_cent\" + 0.004*\"made\" + 0.003*\"united_state\" + 0.003*\"committee\"\n",
      "\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m start_time_iter \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Start time for this iteration\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Train the LDA model with the current set of hyperparameters\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m lda_model, dictionary, tfidf_corpus \u001b[38;5;241m=\u001b[39m train_lda_model(documents_with_bigrams, num_topics\u001b[38;5;241m=\u001b[39mnum_topics, no_below\u001b[38;5;241m=\u001b[39mno_below, no_above\u001b[38;5;241m=\u001b[39mno_above, total_passes\u001b[38;5;241m=\u001b[39mtotal_passes, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, low_value\u001b[38;5;241m=\u001b[39mlow_value)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate Coherence Score using c_v measure\u001b[39;00m\n\u001b[0;32m     24\u001b[0m coherence_model_lda \u001b[38;5;241m=\u001b[39m CoherenceModel(model\u001b[38;5;241m=\u001b[39mlda_model, texts\u001b[38;5;241m=\u001b[39mdocuments_with_bigrams, dictionary\u001b[38;5;241m=\u001b[39mdictionary, coherence\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_v\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 31\u001b[0m, in \u001b[0;36mtrain_lda_model\u001b[1;34m(documents, num_topics, no_below, no_above, total_passes, random_state, low_value)\u001b[0m\n\u001b[0;32m     28\u001b[0m tfidf_corpus \u001b[38;5;241m=\u001b[39m [[(\u001b[38;5;28mid\u001b[39m, freq) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, freq \u001b[38;5;129;01min\u001b[39;00m doc \u001b[38;5;28;01mif\u001b[39;00m tfidf\u001b[38;5;241m.\u001b[39midfs[\u001b[38;5;28mid\u001b[39m] \u001b[38;5;241m>\u001b[39m low_value] \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m corpus]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Initialize and train the LDA model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m LdaModel(corpus\u001b[38;5;241m=\u001b[39mtfidf_corpus, num_topics\u001b[38;5;241m=\u001b[39mnum_topics, id2word\u001b[38;5;241m=\u001b[39mdictionary,\n\u001b[0;32m     32\u001b[0m                      passes\u001b[38;5;241m=\u001b[39mtotal_passes, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lda_model, dictionary, tfidf_corpus\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\ldamodel.py:521\u001b[0m, in \u001b[0;36mLdaModel.__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    519\u001b[0m use_numpy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    520\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 521\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(corpus, chunks_as_numpy\u001b[38;5;241m=\u001b[39muse_numpy)\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreated\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    524\u001b[0m     msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    525\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\ldamodel.py:1006\u001b[0m, in \u001b[0;36mLdaModel.update\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1002\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   1003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPROGRESS: pass \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, at document #\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1004\u001b[0m         pass_, chunk_no \u001b[38;5;241m*\u001b[39m chunksize \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk), lencorpus\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[1;32m-> 1006\u001b[0m     gammat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_estep(chunk, other)\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize_alpha:\n\u001b[0;32m   1009\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_alpha(gammat, rho())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\ldamodel.py:768\u001b[0m, in \u001b[0;36mLdaModel.do_estep\u001b[1;34m(self, chunk, state)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    767\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n\u001b[1;32m--> 768\u001b[0m gamma, sstats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(chunk, collect_sstats\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    769\u001b[0m state\u001b[38;5;241m.\u001b[39msstats \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sstats\n\u001b[0;32m    770\u001b[0m state\u001b[38;5;241m.\u001b[39mnumdocs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m gamma\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# avoids calling len(chunk) on a generator\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\gensim\\models\\ldamodel.py:711\u001b[0m, in \u001b[0;36mLdaModel.inference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    706\u001b[0m expElogbetad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpElogbeta[:, ids]\n\u001b[0;32m    708\u001b[0m \u001b[38;5;66;03m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\u001b[39;00m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# phinorm is the normalizer.\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;66;03m# TODO treat zeros explicitly, instead of adding epsilon?\u001b[39;00m\n\u001b[1;32m--> 711\u001b[0m phinorm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(expElogthetad, expElogbetad) \u001b[38;5;241m+\u001b[39m epsilon\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# Iterate between gamma and phi until convergence\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\multiarray.py:741\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(a, b, out)\u001b[0m\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;124;03m    result_type(*arrays_and_dtypes)\u001b[39;00m\n\u001b[0;32m    673\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    736\u001b[0m \n\u001b[0;32m    737\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays_and_dtypes\n\u001b[1;32m--> 741\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mdot)\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(a, b, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    744\u001b[0m \u001b[38;5;124;03m    dot(a, b, out=None)\u001b[39;00m\n\u001b[0;32m    745\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    829\u001b[0m \n\u001b[0;32m    830\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b, out)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set fixed hyperparameters\n",
    "num_topics = 5\n",
    "total_passes = 3\n",
    "\n",
    "# Define the range of hyperparameters to explore\n",
    "no_below_range = [15, 20, 25]  # Example: Minimum document frequency\n",
    "no_above_range = [0.1, 0.2, 0.3]  # Example: Maximum document frequency proportion\n",
    "low_value_range = [0.01, 0.05, 0.1]  # TF-IDF low value cut-off\n",
    "\n",
    "# Initialize a list to store the results\n",
    "results = []\n",
    "\n",
    "# Perform grid search with timing and topic printing\n",
    "for no_below in no_below_range:\n",
    "    for no_above in no_above_range:\n",
    "        for low_value in low_value_range:\n",
    "            start_time_iter = time.time()  # Start time for this iteration\n",
    "            \n",
    "            # Train the LDA model with the current set of hyperparameters\n",
    "            \n",
    "            lda_model, dictionary, tfidf_corpus = train_lda_model(documents_with_bigrams, num_topics=num_topics, no_below=no_below, no_above=no_above, total_passes=total_passes, random_state=100, low_value=low_value)\n",
    "\n",
    "            # Calculate Coherence Score using c_v measure\n",
    "            coherence_model_lda = CoherenceModel(model=lda_model, texts=documents_with_bigrams, dictionary=dictionary, coherence='c_v')\n",
    "            coherence_lda_cv = coherence_model_lda.get_coherence()\n",
    "            \n",
    "            end_time_iter = time.time()  # End time for this iteration\n",
    "            iter_duration = round((end_time_iter - start_time_iter) / 60)\n",
    "            \n",
    "            # Store the results with iteration time\n",
    "            results.append({\n",
    "                'no_below': no_below,\n",
    "                'no_above': no_above,\n",
    "                'low_value': low_value,\n",
    "                'coherence_score': coherence_lda_cv,\n",
    "                'iteration_time_mins': iter_duration\n",
    "            })\n",
    "            \n",
    "            print(f\"no_below: {no_below}, no_above: {no_above}, low_value: {low_value}, Coherence: {coherence_lda_cv}, Time: {iter_duration} minutes\")\n",
    "            \n",
    "            # Print topics for the current model\n",
    "            print(\"Topics for the current model:\")\n",
    "            for idx, topic in lda_model.print_topics(-1):\n",
    "                print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n",
    "            print(\"-\"*20)\n",
    "\n",
    "# Find and print the best result\n",
    "best_result = max(results, key=lambda x: x['coherence_score'])\n",
    "print(\"\\nBest Model's Params: \", best_result)\n",
    "print(\"Best Coherence Score: \", best_result['coherence_score'])\n",
    "print(\"Time for Best Model: \", best_result['iteration_time_mins'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
